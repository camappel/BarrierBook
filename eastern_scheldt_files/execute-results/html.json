{
  "hash": "f4fb09b76ff14effcae3e20e325bbe69",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Eastern Scheldt Barrier\"\n---\n\n\n\n\n\nThis document provides a complete analysis workflow for the Eastern Scheldt barrier system, combining barrier closure analysis, tide gauge data processing, tidal analysis, and visualization. The analysis covers the period from 1986 to 2024 and includes:\n\n1. **Barrier Closure Analysis**: Loading and analyzing past barrier closure dates, counting closures per water year, and calculating statistics\n2. **Tide Gauge Data Processing**: Loading and combining tide gauge data from multiple sources (OS4 and RPBU gauges)\n3. **Tidal Analysis**: Performing harmonic tidal decomposition and generating predicted astronomical tides\n4. **Visualization**: Creating comprehensive visualizations showing predicted high waters, observed water levels, and barrier closures\n\nWater years run from July 1 to June 30, which is more appropriate for coastal flood analysis than calendar years.\n\n---\n\n# Shared Utilities and Configuration\n\nThis section defines shared utilities, configuration variables, and helper functions used throughout the analysis.\n\n::: {#shared-utilities .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport os\n\n# Shared configuration\noutput_dir = 'output'\ndata_dir = '../2_DATA'\nos.makedirs(output_dir, exist_ok=True)\n\n# Spatial bounds for maps\nXB_GTSM = [-10, 10]  # GTSM longitude bounds\nYB_GTSM = [45, 60]   # GTSM latitude bounds\nXB_MET = [-30, 15]   # ERA5 longitude bounds\nYB_MET = [40, 70]    # ERA5 latitude bounds\n\n# Time intervals for spatial plots\nTINT = 6   # Time interval for GTSM plots (hours)\nTINT_MET = 12  # Time interval for meteorological plots (hours)\n\ndef load_master1_data():\n    \"\"\"Load barrier closure data from mast1.pkl\"\"\"\n    mast1_file = os.path.join(output_dir, 'mast1.pkl')\n    if not os.path.exists(mast1_file):\n        raise FileNotFoundError(\n            f\"Required file {mast1_file} not found. \"\n            \"Please run Part 1 (Barrier Closure Analysis) first to generate the required data file.\"\n        )\n    with open(mast1_file, 'rb') as f:\n        data = pickle.load(f)\n    return data['OCD']  # Observed closure dates\n\ndef load_master3_data():\n    \"\"\"Load tide gauge and tidal prediction data from mast3.pkl\"\"\"\n    mast3_file = os.path.join(output_dir, 'mast3.pkl')\n    if not os.path.exists(mast3_file):\n        raise FileNotFoundError(\n            f\"Required file {mast3_file} not found. \"\n            \"Please run Part 1, Part 2, and Part 3 first to generate the required data files.\"\n        )\n    with open(mast3_file, 'rb') as f:\n        data = pickle.load(f)\n    return {\n        'TSP': np.array(data['TSP']),\n        'TIP': np.array(data['TIP']),\n        'WLP': np.array(data['WLP'])\n    }\n\ndef load_all_data():\n    \"\"\"Load both master1 and master3 data, return as dict\"\"\"\n    OCD = load_master1_data()\n    OCD = np.array(OCD)\n    data3 = load_master3_data()\n    return {\n        'OCD': OCD,\n        'TSP': data3['TSP'],\n        'TIP': data3['TIP'],\n        'WLP': data3['WLP']\n    }\n\ndef create_storms_df(OCD):\n    \"\"\"Create storms DataFrame from closure dates\"\"\"\n    storms_df = pd.DataFrame({\n        'Closure Date': [d.strftime('%Y-%m-%d %H:%M') if isinstance(d, datetime) else str(d) for d in OCD],\n        'Year': [d.year if isinstance(d, datetime) else None for d in OCD],\n        'Month': [d.strftime('%B') if isinstance(d, datetime) else None for d in OCD],\n        'YearMonth': [(d.year, d.month) if isinstance(d, datetime) else None for d in OCD]\n    })\n    \n    # Add water year information\n    def get_water_year(date):\n        \"\"\"Determine water year (July 1 to June 30)\"\"\"\n        if isinstance(date, datetime):\n            if date.month >= 7:\n                return f\"{date.year}/{str(date.year + 1)[2:]}\"\n            else:\n                return f\"{date.year - 1}/{str(date.year)[2:]}\"\n        return None\n    \n    storms_df['Water Year'] = [get_water_year(d) for d in OCD]\n    \n    # Create Event index that groups storms by year and month\n    unique_year_months = storms_df['YearMonth'].unique()\n    year_month_to_index = {ym: idx + 1 for idx, ym in enumerate(sorted(unique_year_months))}\n    storms_df['Event'] = storms_df['YearMonth'].map(year_month_to_index)\n    \n    # Reorder columns\n    storms_df = storms_df[['Event', 'Closure Date', 'Water Year', 'Year', 'Month']]\n    \n    return storms_df\n\ndef get_or_create_storms_df():\n    \"\"\"Get storms_df from pickle file or create it if it doesn't exist\"\"\"\n    storms_df_file = os.path.join(output_dir, 'storms_df.pkl')\n    \n    if os.path.exists(storms_df_file):\n        with open(storms_df_file, 'rb') as f:\n            return pickle.load(f)\n    else:\n        # Load OCD and create storms_df\n        OCD = load_master1_data()\n        storms_df = create_storms_df(OCD)\n        # Save for future use\n        with open(storms_df_file, 'wb') as f:\n            pickle.dump(storms_df, f)\n        return storms_df\n\nprint(\"Shared utilities and configuration loaded\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShared utilities and configuration loaded\n```\n:::\n:::\n\n\n---\n\n# Barrier Closures\n\nThis section analyzes past closures of the Eastern Scheldt barrier. The analysis:\n\n1. Loads barrier closure dates from an Excel file\n2. Counts closures per water year (July 1 to July 1)\n3. Calculates statistics (min, mean, max, total closures)\n4. Creates a bar chart visualization\n\n::: {#setup-master1 .cell execution_count=2}\n``` {.python .cell-code}\n# Configuration - years from 1986 to 2024\nY = list(range(1986, 2025))  # [1986, 1987, ..., 2024]\n\nprint(f\"Analysis configuration:\")\nprint(f\"  Water years: {Y[0]}/{str(Y[0]+1)[2:]} to {Y[-1]}/{str(Y[-1]+1)[2:]} ({len(Y)} years)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis configuration:\n  Water years: 1986/87 to 2024/25 (39 years)\n```\n:::\n:::\n\n\n## Analysis\n\n::: {#load-closures .cell execution_count=3}\n``` {.python .cell-code}\n# 1. Observed closure data - Eastern Scheldt\ndata_dir = '../2_DATA/1_BARRIER_CLOSURES'\nexcel_file = os.path.join(data_dir, 'Eastern_Scheldt_Barrier_Past_Closures_2024.xlsx')\ndf = pd.read_excel(excel_file, sheet_name='Closures')\n\n# Extract closure dates (date only) from column 2 (index 1)\nclosure_dates_col = df.iloc[:, 1]  # Column 2 (0-indexed)\n\n# Extract closure times (should be the 'Start of closure' column)\n# Let's assume the time is in column 3 (index 2). Adjust if necessary.\nif df.shape[1] > 2:\n    closure_times_col = df.iloc[:, 3]  # Column 4 (Start of closure TIME/hh:mm if present)\nelse:\n    # If there is no time column, fallback to NaT\n    closure_times_col = [pd.NaT] * len(closure_dates_col)\n\n# Combine closure date and time to make closure datetime\nOCD = []\nfor d, t in zip(closure_dates_col, closure_times_col):\n    if pd.isna(d):\n        continue\n    date_val = d.to_pydatetime() if isinstance(d, pd.Timestamp) else d\n    if not pd.isna(t):\n        # t can be a time or datetime or string like \"13:00\"\n        if isinstance(t, pd.Timestamp):\n            time_val = t.time()\n        elif isinstance(t, str):\n            try:\n                time_val = pd.to_datetime(t).time()\n            except Exception:\n                time_val = None\n        elif hasattr(t, 'time'):\n            time_val = t.time()\n        else:\n            time_val = None\n        if time_val is not None:\n            combined_dt = datetime.combine(date_val.date(), time_val)\n        else:\n            combined_dt = date_val\n    else:\n        combined_dt = date_val\n    OCD.append(combined_dt)\n\nprint(f\"Loaded {len(OCD)} closure datetimes\")\nif len(OCD) > 0:\n    print(f\"  First closure: {OCD[0]}\")\n    print(f\"  Last closure: {OCD[-1]}\")\n\n# 2. Count closures per water year (July 1 to July 1)\nOCS = []  # Will store [index, year, year+1, num_closures]\nYT = []   # Will store year labels like \"1986/87\"\n\nfor co, y in enumerate(Y, start=1):\n    # Find closures between July 1 of year y and July 1 of year y+1\n    start_date = datetime(y, 7, 1, 0, 0, 0)\n    end_date = datetime(y + 1, 7, 1, 0, 0, 0)\n    \n    # Count closures in this water year\n    closures_in_year = [d for d in OCD if start_date <= d < end_date]\n    num_closures = len(closures_in_year)\n    \n    OCS.append([co, y, y + 1, num_closures])\n    \n    # Create year label (e.g., \"1986/87\")\n    year_str = str(y + 1)\n    YT.append(f\"{y}/{year_str[2:]}\")\n\nOCS = np.array(OCS)\n\nprint(f\"\\nClosures per water year calculated for {len(OCS)} years\")\n\nprint(f\"  Total closures: {np.sum(OCS[:, 3])}\")\n\n# 3. Calculate statistics\nE = np.array([\n    np.min(OCS[:, 3]),      # Minimum closures per year\n    np.mean(OCS[:, 3]),     # Mean closures per year\n    np.max(OCS[:, 3]),      # Maximum closures per year\n    np.sum(OCS[:, 3])       # Total closures\n])\n\nprint(f\"\\nClosure statistics:\")\nprint(f\"  Min per year: {E[0]:.1f}\")\nprint(f\"  Mean per year: {E[1]:.2f}\")\nprint(f\"  Max per year: {E[2]:.1f}\")\nprint(f\"  Total: {E[3]:.0f}\")\n\n#| label: save-master1\n#| include: true\n\n# 4. Save data\noutput_file = os.path.join(output_dir, 'mast1.pkl')\nwith open(output_file, 'wb') as f:\n    pickle.dump({\n        'OCD': OCD,\n        'OCS': OCS,\n        'E': E,\n        'YT': YT\n    }, f)\nprint(f\"\\nData saved to {output_file}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoaded 31 closure datetimes\n  First closure: 1986-10-20 00:00:00\n  Last closure: 2023-12-21 00:00:00\n\nClosures per water year calculated for 39 years\n  Total closures: 31\n\nClosure statistics:\n  Min per year: 0.0\n  Mean per year: 0.79\n  Max per year: 4.0\n  Total: 31\n\nData saved to output/mast1.pkl\n```\n:::\n:::\n\n\n## Visualization \n\nThis bar chart shows the number of barrier closures per water year from 1986/87 to 2024/25. The Eastern Scheldt barrier closes when water levels exceed a threshold, typically during storm surge events.\n\n::: {#plot-closures .cell fig-height='6' fig-width='12' execution_count=4}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(12, 6))\nax.bar(OCS[:, 0], OCS[:, 3], color='black')\nax.set_xlim(0.2, len(OCS) + 0.8)\nax.set_xticks(range(1, len(OCS) + 1))\nax.set_xticklabels(YT, rotation=90)\nax.set_ylim(0, 5)\nax.set_yticks(range(6))\nax.set_ylabel('Number of closures', fontweight='bold', fontsize=18)\nax.set_title('Eastern Scheldt', fontweight='bold', fontsize=18)\nax.grid(True, alpha=0.3)\nax.tick_params(labelsize=16)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master1_closures.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-display}\n![Number of Eastern Scheldt barrier closures per water year (July 1 to June 30), 1986-2024](eastern_scheldt_files/figure-html/plot-closures-output-1.png){#plot-closures-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFigure saved to output/master1_closures.png\n```\n:::\n\n::: {#plot-closures-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\nThe following table lists all barrier closure dates, which correspond to storm events that triggered the Eastern Scheldt barrier.\n\n::: {#cell-table-storms .cell tbl-cap='Complete list of Eastern Scheldt barrier closures (storm events), 1986-2024' execution_count=5}\n``` {.python .cell-code}\n# Get or create storms DataFrame\nstorms_df = get_or_create_storms_df()\n\n# Save storms_df to mast1.pkl for consistency (update existing file)\nmast1_file = os.path.join(output_dir, 'mast1.pkl')\nif os.path.exists(mast1_file):\n    with open(mast1_file, 'rb') as f:\n        mast1_data = pickle.load(f)\n    mast1_data['storms_df'] = storms_df\n    with open(mast1_file, 'wb') as f:\n        pickle.dump(mast1_data, f)\n\n# Display table\nprint(f\"\\nTotal number of closures: {len(storms_df)}\")\nstorms_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTotal number of closures: 31\n```\n:::\n\n::: {#table-storms .cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Event</th>\n      <th>Closure Date</th>\n      <th>Water Year</th>\n      <th>Year</th>\n      <th>Month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1986-10-20 00:00</td>\n      <td>1986/87</td>\n      <td>1986</td>\n      <td>October</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1986-12-19 00:00</td>\n      <td>1986/87</td>\n      <td>1986</td>\n      <td>December</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1986-12-19 00:00</td>\n      <td>1986/87</td>\n      <td>1986</td>\n      <td>December</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1989-02-14 05:40</td>\n      <td>1988/89</td>\n      <td>1989</td>\n      <td>February</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1990-02-27 23:10</td>\n      <td>1989/90</td>\n      <td>1990</td>\n      <td>February</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>1990-02-27 13:50</td>\n      <td>1989/90</td>\n      <td>1990</td>\n      <td>February</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4</td>\n      <td>1990-02-28 23:40</td>\n      <td>1989/90</td>\n      <td>1990</td>\n      <td>February</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>5</td>\n      <td>1990-03-01 00:10</td>\n      <td>1989/90</td>\n      <td>1990</td>\n      <td>March</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>6</td>\n      <td>1990-09-21 12:00</td>\n      <td>1990/91</td>\n      <td>1990</td>\n      <td>September</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>7</td>\n      <td>1990-12-12 05:40</td>\n      <td>1990/91</td>\n      <td>1990</td>\n      <td>December</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>7</td>\n      <td>1990-12-12 20:10</td>\n      <td>1990/91</td>\n      <td>1990</td>\n      <td>December</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>8</td>\n      <td>1992-11-11 13:40</td>\n      <td>1992/93</td>\n      <td>1992</td>\n      <td>November</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>9</td>\n      <td>1993-01-25 15:30</td>\n      <td>1992/93</td>\n      <td>1993</td>\n      <td>January</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>10</td>\n      <td>1993-02-21 08:40</td>\n      <td>1992/93</td>\n      <td>1993</td>\n      <td>February</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>11</td>\n      <td>1993-11-14 11:10</td>\n      <td>1993/94</td>\n      <td>1993</td>\n      <td>November</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>11</td>\n      <td>1993-11-15 00:40</td>\n      <td>1993/94</td>\n      <td>1993</td>\n      <td>November</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>12</td>\n      <td>1994-01-28 10:40</td>\n      <td>1993/94</td>\n      <td>1994</td>\n      <td>January</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>13</td>\n      <td>1995-01-02 21:40</td>\n      <td>1994/95</td>\n      <td>1995</td>\n      <td>January</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>13</td>\n      <td>1995-01-02 14:20</td>\n      <td>1994/95</td>\n      <td>1995</td>\n      <td>January</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>14</td>\n      <td>1996-10-29 11:40</td>\n      <td>1996/97</td>\n      <td>1996</td>\n      <td>October</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>15</td>\n      <td>2002-10-27 14:30</td>\n      <td>2002/03</td>\n      <td>2002</td>\n      <td>October</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>16</td>\n      <td>2003-12-21 08:30</td>\n      <td>2003/04</td>\n      <td>2003</td>\n      <td>December</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>17</td>\n      <td>2004-02-08 13:20</td>\n      <td>2003/04</td>\n      <td>2004</td>\n      <td>February</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>18</td>\n      <td>2007-11-09 22:40</td>\n      <td>2007/08</td>\n      <td>2007</td>\n      <td>November</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>19</td>\n      <td>2013-12-05 22:50</td>\n      <td>2013/14</td>\n      <td>2013</td>\n      <td>December</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>20</td>\n      <td>2014-10-21 22:30</td>\n      <td>2014/15</td>\n      <td>2014</td>\n      <td>October</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>21</td>\n      <td>2018-01-03 12:10</td>\n      <td>2017/18</td>\n      <td>2018</td>\n      <td>January</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>22</td>\n      <td>2020-02-10 11:09</td>\n      <td>2019/20</td>\n      <td>2020</td>\n      <td>February</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>23</td>\n      <td>2022-01-31 10:23</td>\n      <td>2021/22</td>\n      <td>2022</td>\n      <td>January</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>24</td>\n      <td>2022-02-21 04:46</td>\n      <td>2021/22</td>\n      <td>2022</td>\n      <td>February</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>25</td>\n      <td>2023-12-21 00:00</td>\n      <td>2023/24</td>\n      <td>2023</td>\n      <td>December</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n# Tidal Harmonic Analysis\n\nThis section performs comprehensive tidal harmonic analysis on tide gauge data from the Eastern Scheldt, including data processing, harmonic decomposition, and visualization. The analysis:\n\n1. Loads and processes raw tide gauge data from multiple sources (OS4 and RPBU gauges)\n2. Performs harmonic tidal decomposition using `pytides`\n3. Generates tidal predictions for all years\n4. Compares observed water levels with predicted astronomical tides\n5. Visualizes predicted high waters and barrier closures\n\nThe predicted tides represent only the astronomical component, while observed water levels include both tides and meteorological effects (storm surges, wind setup, etc.).\n\n## Tide Gauge Data Processing\n\nThis section loads and processes raw tide gauge data from the Eastern Scheldt. The analysis:\n\n1. Loads data from multiple tide gauge files (RPBU and OS4 gauges)\n2. Cleans data by removing invalid values\n3. Combines time series from different gauges to create a continuous record\n4. Creates a visualization of the complete water level time series\n\nThe combined time series spans from 1986 to 2023, using OS4 gauge data for the early period (1986-1987) and RPBU gauge data for the main period (1987-2023).\n\n::: {#setup-master2 .cell execution_count=6}\n``` {.python .cell-code}\n# Configuration\ndata_dir = '../2_DATA'\n\nprint(f\"Data directory: {data_dir}\")\nprint(f\"Output directory: {output_dir}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData directory: ../2_DATA\nOutput directory: output\n```\n:::\n:::\n\n\n### Load RPBU 1987-2022 Data\n\nLoad the main tide gauge dataset from the RPBU gauge covering April 1987 to December 2022.\n\n::: {#load-rpbu-1987-2022 .cell execution_count=7}\n``` {.python .cell-code}\n# 1. Load in Data 1987 to 2022 (RPBU gauge)\nfile_in = os.path.join(data_dir, '2_TIDE_GAUGE/ES/RPBU_1987_2022.dat')\nD = np.loadtxt(file_in)\n\n# Create time series: April 15, 1987 to December 31, 2022, 10-minute intervals\nstart_date = datetime(1987, 4, 15, 0, 0, 0)\nend_date = datetime(2022, 12, 31, 23, 50, 0)\nTS1 = pd.date_range(start=start_date, end=end_date, freq='10min').to_pydatetime()\n\n# Extract water level data from column 3 (index 2)\nWL1 = D[:, 2]\n\n# Remove default values (9999 indicates missing)\nWL1[WL1 == 9999] = np.nan\n\n# Convert to meters (from cm)\nWL1 = WL1 / 100.0\n\nprint(f\"Loaded RPBU 1987-2022: {len(TS1):,} data points\")\nprint(f\"  Start: {TS1[0]}\")\nprint(f\"  End: {TS1[-1]}\")\nprint(f\"  Valid data: {np.sum(~np.isnan(WL1)):,} points ({100*np.sum(~np.isnan(WL1))/len(WL1):.1f}%)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoaded RPBU 1987-2022: 1,878,480 data points\n  Start: 1987-04-15 00:00:00\n  End: 2022-12-31 23:50:00\n  Valid data: 1,876,721 points (99.9%)\n```\n:::\n:::\n\n\n### Load RPBU 2023 Data\n\nLoad the 2023 data from the RPBU gauge to extend the time series.\n\n::: {#load-rpbu-2023 .cell execution_count=8}\n``` {.python .cell-code}\n# 2. Load in Data 2023 (RPBU gauge)\nfile_in = os.path.join(data_dir, '2_TIDE_GAUGE/ES/RPBU_2023.dat')\nD = np.loadtxt(file_in)\n\n# Create time series: January 1, 2023 to December 31, 2023, 10-minute intervals\nstart_date = datetime(2023, 1, 1, 0, 0, 0)\nend_date = datetime(2023, 12, 31, 23, 50, 0)\nTS2 = pd.date_range(start=start_date, end=end_date, freq='10min').to_pydatetime()\n\n# Extract water level data from column 3 (index 2)\nWL2 = D[:, 2]\n\n# Remove default values (9999 indicates missing)\nWL2[WL2 == 9999] = np.nan\n\n# Convert to meters (from cm)\nWL2 = WL2 / 100.0\n\nprint(f\"Loaded RPBU 2023: {len(TS2):,} data points\")\nprint(f\"  Start: {TS2[0]}\")\nprint(f\"  End: {TS2[-1]}\")\nprint(f\"  Valid data: {np.sum(~np.isnan(WL2)):,} points ({100*np.sum(~np.isnan(WL2))/len(WL2):.1f}%)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoaded RPBU 2023: 52,560 data points\n  Start: 2023-01-01 00:00:00\n  End: 2023-12-31 23:50:00\n  Valid data: 52,560 points (100.0%)\n```\n:::\n:::\n\n\n### Load OS4 1982-2023 Data\n\nLoad data from the OS4 gauge, which provides coverage for the early period before RPBU data begins.\n\n::: {#load-os4 .cell execution_count=9}\n``` {.python .cell-code}\n# 3. Load in Data 1982 to 2023 for OS4 gauge\nfile_in = os.path.join(data_dir, '2_TIDE_GAUGE/ES/OS4_1982_2023.dat')\nD = np.loadtxt(file_in)\n\n# Create time series: January 1, 1982 to December 31, 2023, 10-minute intervals\nstart_date = datetime(1982, 1, 1, 0, 0, 0)\nend_date = datetime(2023, 12, 31, 23, 50, 0)\nTSO = pd.date_range(start=start_date, end=end_date, freq='10min').to_pydatetime()\n\n# Extract water level data from column 3 (index 2)\nWLO = D[:, 2]\n\n# Remove default values (values outside [-500, 500] are invalid)\nWLO[(WLO < -500) | (WLO > 500)] = np.nan\n\n# Convert to meters (from cm)\nWLO = WLO / 100.0\n\nprint(f\"Loaded OS4 1982-2023: {len(TSO):,} data points\")\nprint(f\"  Start: {TSO[0]}\")\nprint(f\"  End: {TSO[-1]}\")\nprint(f\"  Valid data: {np.sum(~np.isnan(WLO)):,} points ({100*np.sum(~np.isnan(WLO))/len(WLO):.1f}%)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoaded OS4 1982-2023: 2,208,960 data points\n  Start: 1982-01-01 00:00:00\n  End: 2023-12-31 23:50:00\n  Valid data: 2,161,485 points (97.9%)\n```\n:::\n:::\n\n\n### Combine Time Series\n\nCombine the time series from different gauges to create a continuous record. Use OS4 data for the early period (1986-1987) before RPBU starts, then switch to RPBU data for the main period.\n\n::: {#combine-series .cell execution_count=10}\n``` {.python .cell-code}\n# 4. Combine time series\n# Use OS4 data for period 1986-01-01 to 1987-04-15 (before RPBU starts)\nstart_combine = datetime(1986, 1, 1, 0, 0, 0)\nend_combine = datetime(1987, 4, 15, 0, 0, 0)\nmask_early = (TSO >= start_combine) & (TSO < end_combine)\nTSO_early = TSO[mask_early]\nWLO_early = WLO[mask_early]\n\n# Combine: OS4 early period + RPBU 1987-2022 + RPBU 2023\nTSP = np.concatenate([TSO_early, TS1, TS2])\nWLP = np.concatenate([WLO_early, WL1, WL2])\n\n# Create reference time series for checking\nTSCHECK = pd.date_range(\n    start=datetime(1986, 1, 1, 0, 0, 0),\n    end=datetime(2023, 12, 31, 23, 50, 0),\n    freq='10min'\n).to_pydatetime()\n\nprint(f\"\\nCombined time series: {len(TSP):,} data points\")\nprint(f\"  Start: {TSP[0]}\")\nprint(f\"  End: {TSP[-1]}\")\nprint(f\"  Valid data: {np.sum(~np.isnan(WLP)):,} points ({100*np.sum(~np.isnan(WLP))/len(WLP):.1f}%)\")\nprint(f\"  Time span: {(TSP[-1] - TSP[0]).days} days\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCombined time series: 1,998,576 data points\n  Start: 1986-01-01 00:00:00\n  End: 2023-12-31 23:50:00\n  Valid data: 1,996,699 points (99.9%)\n  Time span: 13878 days\n```\n:::\n:::\n\n\nSave the combined time series to a pickle file for use in subsequent analyses.\n\n::: {#save-master2 .cell execution_count=11}\n``` {.python .cell-code}\n# 5. Save data\noutput_file = os.path.join(output_dir, 'mast2.pkl')\nwith open(output_file, 'wb') as f:\n    pickle.dump({\n        'TSP': TSP,\n        'WLP': WLP,\n        'TSCHECK': TSCHECK\n    }, f)\nprint(f\"\\nData saved to {output_file}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nData saved to output/mast2.pkl\n```\n:::\n:::\n\n\n### Visualization \n\nThis figure shows the combined tide gauge water level time series from 1986 to 2023 at 10-minute intervals. The data combines OS4 gauge measurements (1986-1987) and RPBU gauge measurements (1987-2023) for the Eastern Scheldt. Water levels are shown in meters relative to NAP (Normal Amsterdam Peil, the Dutch reference datum). The time series shows tidal variations, storm surges, and long-term water level patterns over the 38-year period.\n\n::: {#plot-time-series .cell fig-height='6' fig-width='14' execution_count=12}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(14, 6))\nax.plot(TSP, WLP, 'b', linewidth=0.5)\nax.set_xlabel('Date', fontweight='bold', fontsize=20)\nax.set_ylabel('Water level (m NAP)', fontweight='bold', fontsize=20)\nax.grid(True, alpha=0.3)\nax.tick_params(labelsize=16)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master2_tide_gauge.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-display}\n![Combined tide gauge water level time series for the Eastern Scheldt, 1986-2023. Data combines OS4 gauge (1986-1987) and RPBU gauge (1987-2023) measurements at 10-minute intervals.](eastern_scheldt_files/figure-html/plot-time-series-output-1.png){#plot-time-series-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFigure saved to output/master2_tide_gauge.png\n```\n:::\n\n::: {#plot-time-series-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n## Tidal Analysis\n\nThis section performs harmonic tidal analysis on tide gauge data from the Eastern Scheldt. The analysis:\n\n1. Calculates data quality for each year (1986-2023)\n2. Performs harmonic tidal decomposition using `pytides` (replacing MATLAB's `t_tide`)\n3. Generates tidal predictions for all years\n4. Compares observed water levels with predicted astronomical tides\n\nThe predicted tides represent only the astronomical component, while observed water levels include both tides and meteorological effects (storm surges, wind setup, etc.).\n\n### Setup and Configuration\n\n::: {#setup-master3 .cell execution_count=13}\n``` {.python .cell-code}\nfrom pytides.tide import Tide\n\n# Configuration\nY = list(range(1986, 2024))  # Years 1986 to 2023\nth = 60  # Data quality threshold (percentage)\nlat = 51.64  # Latitude for tidal analysis\n\nprint(f\"Analysis configuration:\")\nprint(f\"  Years: {Y[0]} to {Y[-1]} ({len(Y)} years)\")\nprint(f\"  Data quality threshold: {th}%\")\nprint(f\"  Latitude: {lat}°\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis configuration:\n  Years: 1986 to 2023 (38 years)\n  Data quality threshold: 60%\n  Latitude: 51.64°\n```\n:::\n:::\n\n\n### Load Tide Gauge Data\n\n::: {#load-data-master3 .cell execution_count=14}\n``` {.python .cell-code}\nprint(\"Loading tide gauge data...\")\nmast2_file = os.path.join(output_dir, 'mast2.pkl')\nif not os.path.exists(mast2_file):\n    raise FileNotFoundError(\n        f\"Required file {mast2_file} not found. \"\n        \"Please run Part 2 (Tide Gauge Data Processing) first to generate the required data file.\"\n    )\nwith open(mast2_file, 'rb') as f:\n    data = pickle.load(f)\n    TSP = data['TSP']\n    WLP = data['WLP']\n\n# Convert to numpy arrays if needed\nTSP = np.array(TSP)\nWLP = np.array(WLP)\n\nprint(f\"Loaded {len(TSP):,} data points\")\nprint(f\"  Start: {TSP[0]}\")\nprint(f\"  End: {TSP[-1]}\")\nprint(f\"  Valid data: {np.sum(~np.isnan(WLP)):,} points ({100*np.sum(~np.isnan(WLP))/len(WLP):.1f}%)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoading tide gauge data...\nLoaded 1,998,576 data points\n  Start: 1986-01-01 00:00:00\n  End: 2023-12-31 23:50:00\n  Valid data: 1,996,699 points (99.9%)\n```\n:::\n:::\n\n\n### Calculate Data Quality Per Year\n\n::: {#data-quality .cell execution_count=15}\n``` {.python .cell-code}\nprint(\"\\nCalculating data quality per year...\")\nDQ = []\n\nfor y in Y:\n    # Find data points in this calendar year (Jan 1 to Jan 1 next year)\n    start_date = datetime(y, 1, 1, 0, 0, 0)\n    end_date = datetime(y + 1, 1, 1, 0, 0, 0)\n    mask = (TSP >= start_date) & (TSP < end_date)\n    j = np.where(mask)[0]\n    \n    if len(j) == 0:\n        quality = 0\n    else:\n        # Count non-NaN values\n        k = np.where(~np.isnan(WLP[j]))[0]\n        quality = (len(k) / len(j)) * 100\n    \n    # Initialize with 3 columns: [year, quality, reference_year]\n    # reference_year will be filled in during tidal analysis\n    DQ.append([y, quality, np.nan])\n\nDQ = np.array(DQ)\n\n# Display summary statistics\nprint(f\"\\nData quality summary:\")\nprint(f\"  Range: {np.min(DQ[:, 1]):.1f}% to {np.max(DQ[:, 1]):.1f}%\")\nprint(f\"  Mean: {np.mean(DQ[:, 1]):.1f}%\")\nprint(f\"  Years with quality >= {th}%: {np.sum(DQ[:, 1] >= th)}/{len(Y)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCalculating data quality per year...\n\nData quality summary:\n  Range: 98.8% to 100.0%\n  Mean: 99.9%\n  Years with quality >= 60%: 38/38\n```\n:::\n:::\n\n\n### Tidal Analysis and Prediction\n\nFor each target year, the analysis determines a reference year, extracts data, performs harmonic decomposition, and generates predictions.\n\n::: {#tidal-analysis .cell execution_count=16}\n``` {.python .cell-code}\nprint(\"\\nPerforming tidal analysis and prediction...\")\nTSP2 = []\nTIP = []\n\nfor co, y in enumerate(Y):\n    print(f\"  Processing year {y} ({co+1}/{len(Y)})...\")\n    \n    # Determine reference year\n    if DQ[co, 1] >= th:\n        # Use target year as reference\n        yr = y\n        DQ[co, 2] = yr\n    else:\n        # Find nearest year with quality >= threshold\n        # Calculate distances from target year\n        distances = np.abs(DQ[:, 0] - y)\n        \n        # Filter to only years with quality >= threshold\n        good_mask = DQ[:, 1] >= th\n        \n        if np.any(good_mask):\n            # Get distances for good years only\n            good_distances = distances[good_mask]\n            good_indices = np.where(good_mask)[0]\n            \n            # Find nearest good year\n            nearest_idx = good_indices[np.argmin(good_distances)]\n            yr = int(DQ[nearest_idx, 0])\n        else:\n            # Fallback: use year with best quality\n            best_idx = np.argmax(DQ[:, 1])\n            yr = int(DQ[best_idx, 0])\n        \n        DQ[co, 2] = yr\n    \n    # Extract reference year data (1 year + 1 day for analysis)\n    ref_start = datetime(yr, 1, 1, 0, 0, 0)\n    ref_end = datetime(yr + 1, 1, 2, 0, 0, 0)  # +1 day\n    \n    mask_ref = (TSP >= ref_start) & (TSP < ref_end)\n    j_ref = np.where(mask_ref)[0]\n    \n    if len(j_ref) == 0:\n        print(f\"    Warning: No data found for reference year {yr}\")\n        continue\n    \n    # Get water levels and times for reference year\n    WLP_ref = WLP[j_ref]\n    TSP_ref = TSP[j_ref]\n    \n    # Remove NaN values for pytides analysis\n    valid_mask = ~np.isnan(WLP_ref)\n    WLP_clean = WLP_ref[valid_mask]\n    TSP_clean = TSP_ref[valid_mask]\n    \n    if len(WLP_clean) < 1000:  # Need sufficient data for analysis\n        print(f\"    Warning: Insufficient data for reference year {yr} ({len(WLP_clean)} points)\")\n        continue\n    \n    # Perform tidal analysis using pytides\n    try:\n        tide_model = Tide.decompose(\n            heights=WLP_clean,\n            t=TSP_clean\n        )\n        print(f\"    Analyzed {len(WLP_clean):,} points from reference year {yr}\")\n    except Exception as e:\n        print(f\"    Error in tidal analysis for year {y}: {e}\")\n        continue\n    \n    # Generate prediction timestamps for target year (10-minute intervals)\n    pred_start = datetime(y, 1, 1, 0, 0, 0)\n    pred_end = datetime(y, 12, 31, 23, 50, 0)\n    tsp2 = pd.date_range(start=pred_start, end=pred_end, freq='10min').to_pydatetime()\n    \n    # Predict tides\n    try:\n        tip = tide_model.at(tsp2)\n        TIP.extend(tip)\n        TSP2.extend(tsp2)\n        print(f\"    Predicted {len(tip):,} points for year {y}\")\n    except Exception as e:\n        print(f\"    Error in prediction for year {y}: {e}\")\n        continue\n\n# Convert to numpy arrays\nTSP2 = np.array(TSP2)\nTIP = np.array(TIP)\n\nprint(f\"\\nTotal predictions: {len(TIP):,} points\")\nif len(TIP) > 0:\n    print(f\"  Start: {TSP2[0]}\")\n    print(f\"  End: {TSP2[-1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nPerforming tidal analysis and prediction...\n  Processing year 1986 (1/38)...\n    Analyzed 52,586 points from reference year 1986\n    Predicted 52,560 points for year 1986\n  Processing year 1987 (2/38)...\n    Analyzed 52,704 points from reference year 1987\n    Predicted 52,560 points for year 1987\n  Processing year 1988 (3/38)...\n    Analyzed 52,848 points from reference year 1988\n    Predicted 52,704 points for year 1988\n  Processing year 1989 (4/38)...\n    Analyzed 52,704 points from reference year 1989\n    Predicted 52,560 points for year 1989\n  Processing year 1990 (5/38)...\n    Analyzed 52,704 points from reference year 1990\n    Predicted 52,560 points for year 1990\n  Processing year 1991 (6/38)...\n    Analyzed 52,672 points from reference year 1991\n    Predicted 52,560 points for year 1991\n  Processing year 1992 (7/38)...\n    Analyzed 52,829 points from reference year 1992\n    Predicted 52,704 points for year 1992\n  Processing year 1993 (8/38)...\n    Analyzed 52,704 points from reference year 1993\n    Predicted 52,560 points for year 1993\n  Processing year 1994 (9/38)...\n    Analyzed 52,519 points from reference year 1994\n    Predicted 52,560 points for year 1994\n  Processing year 1995 (10/38)...\n    Analyzed 52,594 points from reference year 1995\n    Predicted 52,560 points for year 1995\n  Processing year 1996 (11/38)...\n    Analyzed 52,848 points from reference year 1996\n    Predicted 52,704 points for year 1996\n  Processing year 1997 (12/38)...\n    Analyzed 52,651 points from reference year 1997\n    Predicted 52,560 points for year 1997\n  Processing year 1998 (13/38)...\n    Analyzed 52,067 points from reference year 1998\n    Predicted 52,560 points for year 1998\n  Processing year 1999 (14/38)...\n    Analyzed 52,696 points from reference year 1999\n    Predicted 52,560 points for year 1999\n  Processing year 2000 (15/38)...\n    Analyzed 52,848 points from reference year 2000\n    Predicted 52,704 points for year 2000\n  Processing year 2001 (16/38)...\n    Analyzed 52,704 points from reference year 2001\n    Predicted 52,560 points for year 2001\n  Processing year 2002 (17/38)...\n    Analyzed 52,704 points from reference year 2002\n    Predicted 52,560 points for year 2002\n  Processing year 2003 (18/38)...\n    Analyzed 52,627 points from reference year 2003\n    Predicted 52,560 points for year 2003\n  Processing year 2004 (19/38)...\n    Analyzed 52,820 points from reference year 2004\n    Predicted 52,704 points for year 2004\n  Processing year 2005 (20/38)...\n    Analyzed 52,651 points from reference year 2005\n    Predicted 52,560 points for year 2005\n  Processing year 2006 (21/38)...\n    Analyzed 52,694 points from reference year 2006\n    Predicted 52,560 points for year 2006\n  Processing year 2007 (22/38)...\n    Analyzed 52,598 points from reference year 2007\n    Predicted 52,560 points for year 2007\n  Processing year 2008 (23/38)...\n    Analyzed 52,815 points from reference year 2008\n    Predicted 52,704 points for year 2008\n  Processing year 2009 (24/38)...\n    Analyzed 52,607 points from reference year 2009\n    Predicted 52,560 points for year 2009\n  Processing year 2010 (25/38)...\n    Analyzed 52,700 points from reference year 2010\n    Predicted 52,560 points for year 2010\n  Processing year 2011 (26/38)...\n    Analyzed 52,683 points from reference year 2011\n    Predicted 52,560 points for year 2011\n  Processing year 2012 (27/38)...\n    Analyzed 52,832 points from reference year 2012\n    Predicted 52,704 points for year 2012\n  Processing year 2013 (28/38)...\n    Analyzed 52,704 points from reference year 2013\n    Predicted 52,560 points for year 2013\n  Processing year 2014 (29/38)...\n    Analyzed 52,648 points from reference year 2014\n    Predicted 52,560 points for year 2014\n  Processing year 2015 (30/38)...\n    Analyzed 52,601 points from reference year 2015\n    Predicted 52,560 points for year 2015\n  Processing year 2016 (31/38)...\n    Analyzed 52,737 points from reference year 2016\n    Predicted 52,704 points for year 2016\n  Processing year 2017 (32/38)...\n    Analyzed 52,704 points from reference year 2017\n    Predicted 52,560 points for year 2017\n  Processing year 2018 (33/38)...\n    Analyzed 52,704 points from reference year 2018\n    Predicted 52,560 points for year 2018\n  Processing year 2019 (34/38)...\n    Analyzed 52,704 points from reference year 2019\n    Predicted 52,560 points for year 2019\n  Processing year 2020 (35/38)...\n    Analyzed 52,848 points from reference year 2020\n    Predicted 52,704 points for year 2020\n  Processing year 2021 (36/38)...\n    Analyzed 52,704 points from reference year 2021\n    Predicted 52,560 points for year 2021\n  Processing year 2022 (37/38)...\n    Analyzed 52,704 points from reference year 2022\n    Predicted 52,560 points for year 2022\n  Processing year 2023 (38/38)...\n    Analyzed 52,560 points from reference year 2023\n    Predicted 52,560 points for year 2023\n\nTotal predictions: 1,998,576 points\n  Start: 1986-01-01 00:00:00\n  End: 2023-12-31 23:50:00\n```\n:::\n:::\n\n\n::: {#save-master3 .cell execution_count=17}\n``` {.python .cell-code}\noutput_file = os.path.join(output_dir, 'mast3.pkl')\nwith open(output_file, 'wb') as f:\n    pickle.dump({\n        'TSP': TSP,\n        'WLP': WLP,\n        'TIP': TIP,\n        'TSP2': TSP2,\n        'DQ': DQ\n    }, f)\nprint(f\"Data saved to {output_file}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData saved to output/mast3.pkl\n```\n:::\n:::\n\n\n### Visualization \n\nThis figure compares observed water levels (blue) with predicted astronomical tides (red) from harmonic analysis. The predicted tides represent the astronomical component only, while observed water levels include both tides and meteorological effects.\n\n::: {#plot-comparison .cell fig-height='6' fig-width='14' execution_count=18}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(14, 6))\nax.plot(TSP2, TIP, 'r', linewidth=0.5, label='Predicted tide', alpha=0.7)\nax.plot(TSP, WLP, 'b', linewidth=0.5, label='Observed water level', alpha=0.7)\nax.set_xlabel('Date', fontweight='bold', fontsize=16)\nax.set_ylabel('Level (m NAP)', fontweight='bold', fontsize=16)\nax.legend(fontsize=14)\nax.grid(True, alpha=0.3)\nax.tick_params(labelsize=14)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master3_tidal_analysis.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-display}\n![Comparison of observed water levels (blue) and predicted astronomical tides (red) for the Eastern Scheldt, 1986-2023](eastern_scheldt_files/figure-html/plot-comparison-output-1.png){#plot-comparison-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFigure saved to output/master3_tidal_analysis.png\n```\n:::\n\n::: {#plot-comparison-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n## Predicted High Waters and Barrier Closures\n\nThis section plots time-series of predicted high waters and barrier closures for the Eastern Scheldt. The analysis:\n\n1. Loads barrier closure dates and tide gauge data from previous analyses\n2. Creates a visualization showing:\n   - Barrier closure dates as vertical dashed lines\n   - Predicted astronomical tides (TIP)\n   - Observed water levels (WLP)\n   - The difference between observed and predicted levels (surge component)\n\nThis visualization helps identify when barrier closures occurred relative to predicted high waters and actual water levels.\n\n::: {#load-data-master4 .cell execution_count=19}\n``` {.python .cell-code}\n# Load all data using shared function\nprint(\"Loading data...\")\ndata = load_all_data()\nOCD = data['OCD']\nTSP = data['TSP']\nTIP = data['TIP']\nWLP = data['WLP']\n\nprint(f\"\\nData loaded:\")\nprint(f\"  Closure dates: {len(OCD)} closures\")\nif len(OCD) > 0:\n    print(f\"    First closure: {OCD[0]}\")\n    print(f\"    Last closure: {OCD[-1]}\")\nprint(f\"  Time series points: {len(TSP):,}\")\nprint(f\"  Predicted tides: {len(TIP):,}\")\nprint(f\"  Observed water levels: {len(WLP):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoading data...\n\nData loaded:\n  Closure dates: 31 closures\n    First closure: 1986-10-20 00:00:00\n    Last closure: 2023-12-21 00:00:00\n  Time series points: 1,998,576\n  Predicted tides: 1,998,576\n  Observed water levels: 1,998,576\n```\n:::\n:::\n\n\n## Visualization \n\nThis figure shows predicted astronomical tides (red), observed water levels (blue), the difference between them (green), and barrier closure dates (vertical dashed magenta lines). The difference (WLP-TIP) represents the non-tidal component, primarily storm surge.\n\n::: {#plot-predicted-high-waters .cell fig-height='8' fig-width='14' execution_count=20}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Plot vertical lines for barrier closure dates\nfor i in range(len(OCD)):\n    ax.axvline(OCD[i], color='m', linestyle='--', linewidth=2, alpha=0.7)\n\n# Plot predicted tides (TIP)\nax.plot(TSP, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\n\n# Plot observed water levels (WLP)\nax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\n\n# Plot difference (WLP - TIP) - surge component\nax.plot(TSP, WLP - TIP, 'g', linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\n\n# Formatting\nax.grid(True, alpha=0.3)\nax.set_xlabel('Date', fontweight='bold', fontsize=16)\nax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\nax.set_ylim(-3, 4)\nax.legend(fontsize=14, loc='best')\nax.tick_params(labelsize=14)\n\nplt.tight_layout()\nplt.show()\n\n# Save figure\nfig_file = os.path.join(output_dir, 'master4_predicted_high_waters.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"\\nFigure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-display}\n![Predicted high waters, observed water levels, and barrier closures for the Eastern Scheldt](eastern_scheldt_files/figure-html/plot-predicted-high-waters-output-1.png){#plot-predicted-high-waters-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFigure saved to output/master4_predicted_high_waters.png\n```\n:::\n\n::: {#plot-predicted-high-waters-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n## Detailed Storm Water Level Analysis\n\nThis section provides a detailed analysis of water levels and surge for Storm 5, Storm 6, Storm 7, Storm 8, Storm 9, Storm 10, Storm 11, Storm 12, Storm 13, Storm 18 (2007), and Storm 19 for the Eastern Scheldt, converting the MATLAB script `master5.m` to Python. The analysis:\n\n1. Loads barrier closure dates and tide gauge data from previous analyses\n2. Calculates surge component (WLP - TIP)\n3. Creates detailed water level visualizations with closure date markers for each storm:\n   - Zoomed water level and surge time series with closure markers (5-day window)\n   - Detailed two-panel view showing water level vs. tide and surge separately\n\nThis provides a comprehensive view of the water level conditions that led to the barrier closures for Storms 5, 6, 7, 8, 9, 10, 11, 12, 13, 18, and 19.\n\n## Setup\n\n::: {#load-closure-tide-data .cell execution_count=21}\n``` {.python .cell-code}\n# Load all data using shared functions\nprint(\"Loading data...\")\ndata = load_all_data()\nOCD = data['OCD']\nTSP = data['TSP']\nTIP = data['TIP']\nWLP = data['WLP']\n\n# Calculate surge\nSUP = WLP - TIP\n\n# Get storms DataFrame\nstorms_df = get_or_create_storms_df()\n\nprint(f\"\\nData loaded:\")\nprint(f\"  Total closure dates: {len(OCD)} closures\")\nprint(f\"  Time series points: {len(TSP):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoading data...\n\nData loaded:\n  Total closure dates: 31 closures\n  Time series points: 1,998,576\n```\n:::\n:::\n\n\n# Spatial Surge and Meteorological Maps\n\nThis document provides spatial analysis of surge patterns and meteorological conditions for Storm 8 (closure event index 11), Storm 18 (closure event index 23, 2007), and Storm 19 (closure event index 24) for the Eastern Scheldt, converting the MATLAB script `master6.m` to Python. The analysis:\n\n1. Loads barrier closure dates to identify the storm events\n2. Loads GTSM reanalysis surge data (spatial surge patterns) for each storm\n3. Loads ERA5 meteorological reanalysis data (pressure and wind fields) for each storm\n4. Creates multiple visualizations for each storm:\n   - GTSM surge maps at 6 time steps around the closure\n   - ERA5 meteorological maps (pressure + wind vectors) at 6 time steps\n\nThis provides a comprehensive view of the spatial patterns and meteorological conditions that led to the barrier closures for Storms 8, 18, and 19.\n\n::: {#setup-maps .cell execution_count=22}\n``` {.python .cell-code}\nimport xarray as xr\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport matplotlib.gridspec as gridspec\n\nprint(f\"Analysis configuration:\")\nprint(f\"  Storms to process: Storm 8 (e=11, prefix '8_'), Storm 18 (e=23, prefix '18_'), and Storm 19 (e=24, prefix '19_')\")\nprint(f\"  GTSM time interval: {TINT} hours\")\nprint(f\"  Meteorological time interval: {TINT_MET} hours\")\nprint(f\"  Output directory: {output_dir}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis configuration:\n  Storms to process: Storm 8 (e=11, prefix '8_'), Storm 18 (e=23, prefix '18_'), and Storm 19 (e=24, prefix '19_')\n  GTSM time interval: 6 hours\n  Meteorological time interval: 12 hours\n  Output directory: output\n```\n:::\n:::\n\n\n::: {#load-closure-data .cell execution_count=23}\n``` {.python .cell-code}\n# Load barrier closure data using shared function\nOCD = load_master1_data()\nOCD = np.array(OCD)\n\n# Validate event indices for all storms\ne_storm8 = 11\ne_storm18 = 23\ne_storm19 = 24\nif e_storm8 >= len(OCD):\n    raise ValueError(f\"Event index {e_storm8} (Storm 8) is out of range. There are {len(OCD)} closure events (indices 0-{len(OCD)-1})\")\nif e_storm18 >= len(OCD):\n    raise ValueError(f\"Event index {e_storm18} (Storm 18) is out of range. There are {len(OCD)} closure events (indices 0-{len(OCD)-1})\")\nif e_storm19 >= len(OCD):\n    raise ValueError(f\"Event index {e_storm19} (Storm 19) is out of range. There are {len(OCD)} closure events (indices 0-{len(OCD)-1})\")\n\nprint(f\"\\nData loaded:\")\nprint(f\"  Total closure dates: {len(OCD)} closures\")\nprint(f\"  Storm 8 closure date: {OCD[e_storm8]}\")\nprint(f\"  Storm 18 closure date: {OCD[e_storm18]}\")\nprint(f\"  Storm 19 closure date: {OCD[e_storm19]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nData loaded:\n  Total closure dates: 31 closures\n  Storm 8 closure date: 1992-11-11 13:40:00\n  Storm 18 closure date: 2007-11-09 22:40:00\n  Storm 19 closure date: 2013-12-05 22:50:00\n```\n:::\n:::\n\n\n# Storms \n\n## Storm 1\n\nStorm 1 uses datasets with prefix \"1_\" in the 2_DATA directory. The storm number corresponds directly to the row index in the closure dates table.\n\n### Water Level and Surge Analysis\n\n::: {#plot-predicted-high-waters-zoomed-storm1 .cell fig-height='8' fig-width='14' execution_count=24}\n``` {.python .cell-code}\n# Load data\ndata = load_all_data()\nOCD = data['OCD']\nTSP = data['TSP']\nTIP = data['TIP']\nWLP = data['WLP']\nstorms_df = get_or_create_storms_df()\n\n# First storm: closure index 0, prefix '1_'\ne = 0\nclosure_date = OCD[e]\nclosure_dates = [closure_date]\nclosure_dates_plus_one = [closure_date + timedelta(days=1)]\n\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Plot vertical lines for closure date\nax.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\nax.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\n\n# Plot predicted tides, observed water levels, and surge\nax.plot(TSP, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\nax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\nax.plot(TSP, WLP - TIP, 'g', linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\n\n# Set zoom window: 2 days before to 3 days after closure\nax.set_xlim(closure_date - timedelta(days=2), closure_date + timedelta(days=3))\nax.set_ylim(-3, 4)\nax.grid(True, alpha=0.3)\nax.set_xlabel('Date', fontweight='bold', fontsize=16)\nax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\nax.set_title('Storm 1 - Predicted High Waters (Zoomed View)', fontweight='bold', fontsize=18)\nax.legend(fontsize=14, loc='best')\nax.tick_params(labelsize=14)\n\nplt.tight_layout()\nplt.show()\n\nfig_file = os.path.join(output_dir, 'high_waters_storm1_predicted_high_waters_zoomed.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"Figure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-display}\n![Predicted high waters, observed water levels, and barrier closure (zoomed view) for Storm 1 - Eastern Scheldt](eastern_scheldt_files/figure-html/plot-predicted-high-waters-zoomed-storm1-output-1.png){#plot-predicted-high-waters-zoomed-storm1-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFigure saved to output/high_waters_storm1_predicted_high_waters_zoomed.png\n```\n:::\n\n::: {#plot-predicted-high-waters-zoomed-storm1-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n::: {#plot-water-level-surge-storm1 .cell fig-height='10' fig-width='14' execution_count=25}\n``` {.python .cell-code}\n# First storm: closure index 0\ne = 0\nclosure_date = OCD[e]\nclosure_dates_plus_one = [closure_date + timedelta(days=1)]\n\n# Calculate surge\nSUP = WLP - TIP\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# Top panel: Water level and predicted tide\nax1.plot(TSP, WLP, 'b', linewidth=2, label='Water level')\nax1.plot(TSP, TIP, 'r', linewidth=2, label='Astronomical tide')\nax1.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\nax1.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\nax1.set_ylabel('Water level (m)', fontweight='bold', fontsize=20)\nax1.set_title('Storm 1 - Water Level and Surge', fontweight='bold', fontsize=18)\nax1.legend(fontsize=14)\nax1.grid(True, alpha=0.3)\nax1.tick_params(labelsize=16)\nax1.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n\n# Bottom panel: Surge\nsurge_color = np.array([255, 103, 40]) / 255\nax2.plot(TSP, SUP, color=surge_color, linewidth=2)\nax2.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7)\nax2.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7)\nax2.set_xlabel('Date', fontweight='bold', fontsize=20)\nax2.set_ylabel('Surge (m)', fontweight='bold', fontsize=20)\nax2.grid(True, alpha=0.3)\nax2.tick_params(labelsize=16)\nax2.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n\nplt.tight_layout()\nplt.show()\n\nfig_file = os.path.join(output_dir, 'high_waters_storm1_water_level_surge.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"Figure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-display}\n![Water level and surge time series for Storm 1 closure event](eastern_scheldt_files/figure-html/plot-water-level-surge-storm1-output-1.png){#plot-water-level-surge-storm1-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFigure saved to output/high_waters_storm1_water_level_surge.png\n```\n:::\n\n::: {#plot-water-level-surge-storm1-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n### Spatial Surge and Meteorological Maps\n\n::: {#load-spatial-data-storm1 .cell execution_count=26}\n``` {.python .cell-code}\n# Load data\nOCD = load_master1_data()\nOCD = np.array(OCD)\n\n# First storm: closure index 0, prefix '1_'\ne = 0\nprefix = '1_'\nclosure_date = OCD[e]\nclosure_year = closure_date.year\nclosure_month = closure_date.month\ngtsm_start_date = closure_date\nmet_start_date = closure_date - timedelta(hours=6)\n\n# Load GTSM reanalysis surge data\nfile_in = os.path.join(data_dir, '3_CODEC_GTSM', f'{prefix}reanalysis_surge_hourly_{closure_year}_{closure_month:02d}_v3.nc')\nds_gtsm = xr.open_dataset(file_in)\n\nTS_GT = pd.to_datetime(ds_gtsm['time'].values)\nX_GT = ds_gtsm['station_x_coordinate'].values\nY_GT = ds_gtsm['station_y_coordinate'].values\nSU_GT = ds_gtsm['surge'].values\n\nmask = (X_GT >= XB_GTSM[0]) & (X_GT <= XB_GTSM[1]) & (Y_GT >= YB_GTSM[0]) & (Y_GT <= YB_GTSM[1])\nX_GT = X_GT[mask]\nY_GT = Y_GT[mask]\nSU_GT = SU_GT[:, mask]\n\n# Load ERA5 meteorological data\nfilein_met = os.path.join(data_dir, '4_ERA5', f'{prefix}ERA5_{closure_year}_{closure_month:02d}.nc')\nds_met = xr.open_dataset(filein_met)\n\nLON = ds_met['longitude'].values\nLAT = ds_met['latitude'].values\nP_MET = ds_met['msl'].values\nU_MET = ds_met['u10'].values\nV_MET = ds_met['v10'].values\nTS_MET = pd.to_datetime(ds_met['valid_time'].values)\n\n# Reorder to (lon, lat, time)\ndims = ds_met['msl'].dims\nif dims[0] == 'valid_time' and dims[1] == 'latitude' and dims[2] == 'longitude':\n    P_MET = np.transpose(P_MET, (2, 1, 0))\n    U_MET = np.transpose(U_MET, (2, 1, 0))\n    V_MET = np.transpose(V_MET, (2, 1, 0))\n\nX_MET, Y_MET = np.meshgrid(LON, LAT)\nX_MET = X_MET.T\nY_MET = Y_MET.T\n\nlon_mask = (LON >= XB_MET[0]) & (LON <= XB_MET[1])\nlat_mask = (LAT >= YB_MET[0]) & (LAT <= YB_MET[1])\nlon_idx = np.where(lon_mask)[0]\nlat_idx = np.where(lat_mask)[0]\n```\n:::\n\n\n::: {#find-time-indices-storm1 .cell execution_count=27}\n``` {.python .cell-code}\n# Find time indices for GTSM plots\ntime_diffs = np.abs([(t - gtsm_start_date).total_seconds() for t in TS_GT])\nt_gtsm_base = np.argmin(time_diffs)\nt_gtsm = [t_gtsm_base, t_gtsm_base + TINT, t_gtsm_base + TINT*2, \n          t_gtsm_base + TINT*3, t_gtsm_base + TINT*4, t_gtsm_base + TINT*5]\nt_gtsm = [max(0, min(t, len(TS_GT)-1)) for t in t_gtsm]\n\n# Find time indices for meteorological plots\nmax_time_idx = P_MET.shape[2] - 1\nvalid_ts_met = TS_MET[:max_time_idx+1]\ntarget_times = [met_start_date + timedelta(hours=h) for h in [0, 12, 24, 36, 48, 60]]\nt_met = [np.argmin(np.abs([(t - target_time).total_seconds() for t in valid_ts_met])) for target_time in target_times]\n```\n:::\n\n\n::: {#plot-gtsm-surge-storm1 .cell fig-height='10' fig-width='16' execution_count=28}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(16, 10))\ngs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\naxes = [fig.add_subplot(gs[i // 3, i % 3]) for i in range(6)]\n\nfor i, t_idx in enumerate(t_gtsm):\n    ax = axes[i]\n    scatter = ax.scatter(X_GT, Y_GT, c=SU_GT[t_idx, :], cmap='jet', s=15, vmin=-0.5, vmax=1.0)\n    ax.set_xlim(XB_GTSM)\n    ax.set_ylim(YB_GTSM)\n    ax.set_aspect('equal')\n    ax.grid(True, alpha=0.3)\n    ax.tick_params(labelsize=12)\n    ax.set_title(TS_GT[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n    if i >= 3:\n        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n    if i % 3 == 0:\n        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n\ncbar_ax = fig.add_subplot(gs[:, 3])\ncbar = plt.colorbar(scatter, cax=cbar_ax, orientation='vertical')\ncbar.set_label('Surge (m)', fontweight='bold', fontsize=14)\ncbar.ax.tick_params(labelsize=12)\n\nplt.tight_layout(rect=[0, 0, 0.97, 1])\nplt.show()\n\nfig_file = os.path.join(output_dir, 'maps_storm1_gtsm_surge.png')\nplt.savefig(fig_file, dpi=100, bbox_inches='tight')\nprint(f\"Figure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/m8/cp78hgjj6j937vhj63cdm7_c0000gn/T/ipykernel_47601/1400714639.py:24: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.97, 1])\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![GTSM reanalysis surge maps at 6 time steps around Storm 1 closure event](eastern_scheldt_files/figure-html/plot-gtsm-surge-storm1-output-2.png){#plot-gtsm-surge-storm1-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFigure saved to output/maps_storm1_gtsm_surge.png\n```\n:::\n\n::: {#plot-gtsm-surge-storm1-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n::: {#plot-era5-met-storm1 .cell fig-height='10' fig-width='16' execution_count=29}\n``` {.python .cell-code}\nint_skip = 3\nX_sub = X_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\nY_sub = Y_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\nX_vec = X_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\nY_vec = Y_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\n\nfig = plt.figure(figsize=(16, 10))\ngs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\naxes = [fig.add_subplot(gs[i // 3, i % 3], projection=ccrs.PlateCarree()) for i in range(6)]\n\nims = []\nfor i, t_idx in enumerate(t_met):\n    ax = axes[i]\n    ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth=0.8, color='white')\n    ax.add_feature(cfeature.BORDERS.with_scale('50m'), linewidth=0.5, color='white')\n    \n    P_sub = P_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1, t_idx] / 100\n    U_vec = U_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n    V_vec = V_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n    \n    im = ax.pcolormesh(X_sub, Y_sub, P_sub, cmap='jet', shading='gouraud', \n                       vmin=967, vmax=1020, transform=ccrs.PlateCarree())\n    ims.append(im)\n    ax.quiver(X_vec, Y_vec, U_vec, V_vec, color='k', scale=1200, width=0.002, \n              transform=ccrs.PlateCarree())\n    \n    ax.set_xlim(XB_MET)\n    ax.set_ylim(YB_MET)\n    ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.3, linestyle='--')\n    ax.tick_params(labelsize=12)\n    ax.set_title(TS_MET[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n    if i >= 3:\n        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n    if i % 3 == 0:\n        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n\ncbar_ax = fig.add_subplot(gs[:, 3])\ncbar = plt.colorbar(ims[0], cax=cbar_ax, orientation='vertical')\ncbar.set_label('Pressure (hPa)', fontweight='bold', fontsize=14)\ncbar.ax.tick_params(labelsize=12)\n\nplt.tight_layout(rect=[0, 0, 0.97, 1])\nplt.show()\n\nfig_file = os.path.join(output_dir, 'maps_storm1_era5_meteorology.png')\nplt.savefig(fig_file, dpi=100, bbox_inches='tight')\nprint(f\"Figure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/var/folders/m8/cp78hgjj6j937vhj63cdm7_c0000gn/T/ipykernel_47601/3093845481.py:42: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.97, 1])\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![ERA5 reanalysis pressure and wind fields at 6 time steps around Storm 1 closure event](eastern_scheldt_files/figure-html/plot-era5-met-storm1-output-2.png){#plot-era5-met-storm1-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFigure saved to output/maps_storm1_era5_meteorology.png\n```\n:::\n\n::: {#plot-era5-met-storm1-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n## Storm 2\n\nStorm 2 uses datasets with prefix \"2_\" in the 2_DATA directory. The storm number corresponds directly to the row index in the closure dates table.\n\n### Water Level and Surge Analysis\n\n::: {#plot-predicted-high-waters-zoomed-storm2 .cell fig-height='8' fig-width='14' execution_count=30}\n``` {.python .cell-code}\n# Load data\ndata = load_all_data()\nOCD = data['OCD']\nTSP = data['TSP']\nTIP = data['TIP']\nWLP = data['WLP']\n\n# Second storm: closure index 1, prefix '2_'\ne = 1\nclosure_date = OCD[e]\nclosure_dates = [closure_date]\nclosure_dates_plus_one = [closure_date + timedelta(days=1)]\n\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Plot vertical lines for closure date\nax.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\nax.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\n\n# Plot predicted tides, observed water levels, and surge\nax.plot(TSP, TIP, 'r', linewidth=1, label='Predicted tide (TIP)', alpha=0.8)\nax.plot(TSP, WLP, 'b', linewidth=1, label='Observed water level (WLP)', alpha=0.8)\nax.plot(TSP, WLP - TIP, 'g', linewidth=1, label='Surge (WLP - TIP)', alpha=0.8)\n\n# Set zoom window: 2 days before to 3 days after closure\nax.set_xlim(closure_date - timedelta(days=2), closure_date + timedelta(days=3))\nax.set_ylim(-3, 4)\nax.grid(True, alpha=0.3)\nax.set_xlabel('Date', fontweight='bold', fontsize=16)\nax.set_ylabel('Level (m)', fontweight='bold', fontsize=16)\nax.set_title('Storm 2 - Predicted High Waters (Zoomed View)', fontweight='bold', fontsize=18)\nax.legend(fontsize=14, loc='best')\nax.tick_params(labelsize=14)\n\nplt.tight_layout()\nplt.show()\n\nfig_file = os.path.join(output_dir, 'high_waters_storm2_predicted_high_waters_zoomed.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"Figure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-display}\n![Predicted high waters, observed water levels, and barrier closure (zoomed view) for Storm 2 - Eastern Scheldt](eastern_scheldt_files/figure-html/plot-predicted-high-waters-zoomed-storm2-output-1.png){#plot-predicted-high-waters-zoomed-storm2-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFigure saved to output/high_waters_storm2_predicted_high_waters_zoomed.png\n```\n:::\n\n::: {#plot-predicted-high-waters-zoomed-storm2-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n::: {#plot-water-level-surge-storm2 .cell fig-height='10' fig-width='14' execution_count=31}\n``` {.python .cell-code}\n# Second storm: closure index 1\ne = 1\nclosure_date = OCD[e]\nclosure_dates_plus_one = [closure_date + timedelta(days=1)]\n\n# Calculate surge\nSUP = WLP - TIP\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# Top panel: Water level and predicted tide\nax1.plot(TSP, WLP, 'b', linewidth=2, label='Water level')\nax1.plot(TSP, TIP, 'r', linewidth=2, label='Astronomical tide')\nax1.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date')\nax1.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7, label='Closure date + 1 day')\nax1.set_ylabel('Water level (m)', fontweight='bold', fontsize=20)\nax1.set_title('Storm 2 - Water Level and Surge', fontweight='bold', fontsize=18)\nax1.legend(fontsize=14)\nax1.grid(True, alpha=0.3)\nax1.tick_params(labelsize=16)\nax1.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n\n# Bottom panel: Surge\nsurge_color = np.array([255, 103, 40]) / 255\nax2.plot(TSP, SUP, color=surge_color, linewidth=2)\nax2.axvline(closure_date, color='m', linestyle='--', linewidth=2, alpha=0.7)\nax2.axvline(closure_dates_plus_one[0], color='m', linestyle='--', linewidth=2, alpha=0.7)\nax2.set_xlabel('Date', fontweight='bold', fontsize=20)\nax2.set_ylabel('Surge (m)', fontweight='bold', fontsize=20)\nax2.grid(True, alpha=0.3)\nax2.tick_params(labelsize=16)\nax2.set_xlim(closure_date - timedelta(hours=12), closure_date + timedelta(hours=60))\n\nplt.tight_layout()\nplt.show()\n\nfig_file = os.path.join(output_dir, 'high_waters_storm2_water_level_surge.png')\nplt.savefig(fig_file, dpi=150, bbox_inches='tight')\nprint(f\"Figure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-display}\n![Water level and surge time series for Storm 2 closure event](eastern_scheldt_files/figure-html/plot-water-level-surge-storm2-output-1.png){#plot-water-level-surge-storm2-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFigure saved to output/high_waters_storm2_water_level_surge.png\n```\n:::\n\n::: {#plot-water-level-surge-storm2-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n### Spatial Surge and Meteorological Maps\n\n::: {#load-spatial-data-storm2 .cell execution_count=32}\n``` {.python .cell-code}\n# Load data\nOCD = load_master1_data()\nOCD = np.array(OCD)\n\n# Second storm: closure index 1, prefix '2_'\ne = 1\nprefix = '2_'\nclosure_date = OCD[e]\nclosure_year = closure_date.year\nclosure_month = closure_date.month\ngtsm_start_date = closure_date\nmet_start_date = closure_date - timedelta(hours=6)\n\n# Load GTSM reanalysis surge data\nfile_in = os.path.join(data_dir, '3_CODEC_GTSM', f'{prefix}reanalysis_surge_hourly_{closure_year}_{closure_month:02d}_v3.nc')\nds_gtsm = xr.open_dataset(file_in)\n\nTS_GT = pd.to_datetime(ds_gtsm['time'].values)\nX_GT = ds_gtsm['station_x_coordinate'].values\nY_GT = ds_gtsm['station_y_coordinate'].values\nSU_GT = ds_gtsm['surge'].values\n\nmask = (X_GT >= XB_GTSM[0]) & (X_GT <= XB_GTSM[1]) & (Y_GT >= YB_GTSM[0]) & (Y_GT <= YB_GTSM[1])\nX_GT = X_GT[mask]\nY_GT = Y_GT[mask]\nSU_GT = SU_GT[:, mask]\n\n# Load ERA5 meteorological data\nfilein_met = os.path.join(data_dir, '4_ERA5', f'{prefix}ERA5_{closure_year}_{closure_month:02d}.nc')\nds_met = xr.open_dataset(filein_met)\n\nLON = ds_met['longitude'].values\nLAT = ds_met['latitude'].values\nP_MET = ds_met['msl'].values\nU_MET = ds_met['u10'].values\nV_MET = ds_met['v10'].values\nTS_MET = pd.to_datetime(ds_met['valid_time'].values)\n\n# Reorder to (lon, lat, time)\ndims = ds_met['msl'].dims\nif dims[0] == 'valid_time' and dims[1] == 'latitude' and dims[2] == 'longitude':\n    P_MET = np.transpose(P_MET, (2, 1, 0))\n    U_MET = np.transpose(U_MET, (2, 1, 0))\n    V_MET = np.transpose(V_MET, (2, 1, 0))\n\nX_MET, Y_MET = np.meshgrid(LON, LAT)\nX_MET = X_MET.T\nY_MET = Y_MET.T\n\nlon_mask = (LON >= XB_MET[0]) & (LON <= XB_MET[1])\nlat_mask = (LAT >= YB_MET[0]) & (LAT <= YB_MET[1])\nlon_idx = np.where(lon_mask)[0]\nlat_idx = np.where(lat_mask)[0]\n```\n:::\n\n\n::: {#find-time-indices-storm2 .cell execution_count=33}\n``` {.python .cell-code}\n# Find time indices for GTSM plots\ntime_diffs = np.abs([(t - gtsm_start_date).total_seconds() for t in TS_GT])\nt_gtsm_base = np.argmin(time_diffs)\nt_gtsm = [t_gtsm_base, t_gtsm_base + TINT, t_gtsm_base + TINT*2, \n          t_gtsm_base + TINT*3, t_gtsm_base + TINT*4, t_gtsm_base + TINT*5]\nt_gtsm = [max(0, min(t, len(TS_GT)-1)) for t in t_gtsm]\n\n# Find time indices for meteorological plots\nmax_time_idx = P_MET.shape[2] - 1\nvalid_ts_met = TS_MET[:max_time_idx+1]\ntarget_times = [met_start_date + timedelta(hours=h) for h in [0, 12, 24, 36, 48, 60]]\nt_met = [np.argmin(np.abs([(t - target_time).total_seconds() for t in valid_ts_met])) for target_time in target_times]\n```\n:::\n\n\n::: {#plot-gtsm-surge-storm2 .cell fig-height='10' fig-width='16' execution_count=34}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(16, 10))\ngs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\naxes = [fig.add_subplot(gs[i // 3, i % 3]) for i in range(6)]\n\nfor i, t_idx in enumerate(t_gtsm):\n    ax = axes[i]\n    scatter = ax.scatter(X_GT, Y_GT, c=SU_GT[t_idx, :], cmap='jet', s=15, vmin=-0.5, vmax=1.0)\n    ax.set_xlim(XB_GTSM)\n    ax.set_ylim(YB_GTSM)\n    ax.set_aspect('equal')\n    ax.grid(True, alpha=0.3)\n    ax.tick_params(labelsize=12)\n    ax.set_title(TS_GT[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n    if i >= 3:\n        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n    if i % 3 == 0:\n        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n\ncbar_ax = fig.add_subplot(gs[:, 3])\ncbar = plt.colorbar(scatter, cax=cbar_ax, orientation='vertical')\ncbar.set_label('Surge (m)', fontweight='bold', fontsize=14)\ncbar.ax.tick_params(labelsize=12)\n\nplt.tight_layout(rect=[0, 0, 0.97, 1])\nplt.show()\n\nfig_file = os.path.join(output_dir, 'maps_storm2_gtsm_surge.png')\nplt.savefig(fig_file, dpi=100, bbox_inches='tight')\nprint(f\"Figure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/m8/cp78hgjj6j937vhj63cdm7_c0000gn/T/ipykernel_47601/3016337651.py:24: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.97, 1])\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![GTSM reanalysis surge maps at 6 time steps around Storm 2 closure event](eastern_scheldt_files/figure-html/plot-gtsm-surge-storm2-output-2.png){#plot-gtsm-surge-storm2-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFigure saved to output/maps_storm2_gtsm_surge.png\n```\n:::\n\n::: {#plot-gtsm-surge-storm2-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n::: {#plot-era5-met-storm2 .cell fig-height='10' fig-width='16' execution_count=35}\n``` {.python .cell-code}\nint_skip = 3\nX_sub = X_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\nY_sub = Y_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1]\nX_vec = X_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\nY_vec = Y_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip]\n\nfig = plt.figure(figsize=(16, 10))\ngs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.06], wspace=0.3, hspace=0.25)\naxes = [fig.add_subplot(gs[i // 3, i % 3], projection=ccrs.PlateCarree()) for i in range(6)]\n\nims = []\nfor i, t_idx in enumerate(t_met):\n    ax = axes[i]\n    ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth=0.8, color='white')\n    ax.add_feature(cfeature.BORDERS.with_scale('50m'), linewidth=0.5, color='white')\n    \n    P_sub = P_MET[lon_idx[0]:lon_idx[-1]+1, lat_idx[0]:lat_idx[-1]+1, t_idx] / 100\n    U_vec = U_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n    V_vec = V_MET[lon_idx[0]:lon_idx[-1]+1:int_skip, lat_idx[0]:lat_idx[-1]+1:int_skip, t_idx]\n    \n    im = ax.pcolormesh(X_sub, Y_sub, P_sub, cmap='jet', shading='gouraud', \n                       vmin=967, vmax=1020, transform=ccrs.PlateCarree())\n    ims.append(im)\n    ax.quiver(X_vec, Y_vec, U_vec, V_vec, color='k', scale=1200, width=0.002, \n              transform=ccrs.PlateCarree())\n    \n    ax.set_xlim(XB_MET)\n    ax.set_ylim(YB_MET)\n    ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.3, linestyle='--')\n    ax.tick_params(labelsize=12)\n    ax.set_title(TS_MET[t_idx].strftime('%Y-%m-%d %H:%M'), fontweight='bold', fontsize=16)\n    if i >= 3:\n        ax.set_xlabel('Longitude (deg)', fontweight='bold', fontsize=14)\n    if i % 3 == 0:\n        ax.set_ylabel('Latitude (deg)', fontweight='bold', fontsize=14)\n\ncbar_ax = fig.add_subplot(gs[:, 3])\ncbar = plt.colorbar(ims[0], cax=cbar_ax, orientation='vertical')\ncbar.set_label('Pressure (hPa)', fontweight='bold', fontsize=14)\ncbar.ax.tick_params(labelsize=12)\n\nplt.tight_layout(rect=[0, 0, 0.97, 1])\nplt.show()\n\nfig_file = os.path.join(output_dir, 'maps_storm2_era5_meteorology.png')\nplt.savefig(fig_file, dpi=100, bbox_inches='tight')\nprint(f\"Figure saved to {fig_file}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/opt/miniconda3/envs/objective1/lib/python3.14/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning: facecolor will have no effect as it has been defined as \"never\".\n  warnings.warn('facecolor will have no effect as it has been '\n/var/folders/m8/cp78hgjj6j937vhj63cdm7_c0000gn/T/ipykernel_47601/2863873744.py:42: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.97, 1])\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![ERA5 reanalysis pressure and wind fields at 6 time steps around Storm 2 closure event](eastern_scheldt_files/figure-html/plot-era5-met-storm2-output-2.png){#plot-era5-met-storm2-1}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFigure saved to output/maps_storm2_era5_meteorology.png\n```\n:::\n\n::: {#plot-era5-met-storm2-2 .cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n",
    "supporting": [
      "eastern_scheldt_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}